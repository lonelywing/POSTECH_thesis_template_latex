@article{Moin1998,
  author = {Moin, Parviz and Mahesh, Krishnan},
  title = {DIRECT NUMERICAL SIMULATION: A Tool in Turbulence Research},
  journal = {Annual Review of Fluid Mechanics},
  volume = {30},
  number = {1},
  pages = {539-578},
  year = {1998},
  doi = {10.1146/annurev.fluid.30.1.539},
}

@article{PhysRevD.93.124052,
  title = {What is the entropy in entropic gravity?},
  author = {Carroll, Sean M. and Remmen, Grant N.},
  journal = {Phys. Rev. D},
  volume = {93},
  issue = {12},
  pages = {124052},
  numpages = {13},
  year = {2016},
  month = jun,
  publisher = {American Physical Society},
  doi = {10.1103/PhysRevD.93.124052},
  url = {https://link.aps.org/doi/10.1103/PhysRevD.93.124052}
}

@article{pimentel-etal-2020-phonotactic,
  title = "Phonotactic Complexity and Its Trade-offs",
  author = "Pimentel, Tiago  and
    Roark, Brian  and
    Cotterell, Ryan",
  journal = "Transactions of the Association for Computational Linguistics",
  volume = "8",
  year = "2020",
  url = "https://www.aclweb.org/anthology/2020.tacl-1.1",
  doi = "10.1162/tacl_a_00296",
  pages = "1--18",
  abstract = "We present methods for calculating a measure of phonotactic complexity{---}bits per phoneme{---} that permits a straightforward cross-linguistic comparison. When given a word, represented as a sequence of phonemic segments such as symbols in the international phonetic alphabet, and a statistical model trained on a sample of word types from the language, we can approximately measure bits per phoneme using the negative log-probability of that word under the model. This simple measure allows us to compare the entropy across languages, giving insight into how complex a language{'}s phonotactics is. Using a collection of 1016 basic concept words across 106 languages, we demonstrate a very strong negative correlation of âˆ’ 0.74 between bits per phoneme and the average length of words.",
}

@article{jaker_kiparsky_2020,
  title={Level ordering and opacity in {Tets\c{\'o}t'\i{}n\'e}: a Stratal {OT} account},
  volume={37}, DOI={10.1017/S0952675720000299},
  number={4}, journal={Phonology},
  publisher={Cambridge University Press},
  author={Jaker, Alessandro and Kiparsky, Paul},
  year={2020}, pages={617–655}
}

@ARTICLE{726791,
  author={Yann LeCun and L\'eon Bottou and Yoshua Bengio and Patrick Haffner},
  journal={Proceedings of the IEEE},
  title={Gradient-based learning applied to document recognition},
  year={1998},
  volume={86},
  number={11},
  pages={2278-2324},
  doi={10.1109/5.726791}
}

@article{Erdos1960,
  author = {Erd\H{o}s, Paul and R\'enyi, Alfr\'ed},
  journal = {Acta Arithmetica},
  keywords = {number theory},
  language = {eng},
  number = {1},
  pages = {83-110},
  title = {Additive properties of random sequences of positive integers},
  url = {http://eudml.org/doc/206705},
  volume = {6},
  year = {1960},
}

@INPROCEEDINGS{Erdos60onthe,
  author = {Paul Erd\H{o}s and Alfr\'ed R\'enyi},
  title = {On the Evolution of Random Graphs},
  booktitle = {Publication of the Mathematical Institute of the Hungarian Academy of Sciences},
  url={http://citeseerx.ist.psu.edu/viewdoc/summary?doi=10.1.1.153.5943},
  year = {1960},
  pages = {17--61},
  publisher = {}
}

@inproceedings{NIPS1989_53c3bce6,
  author = {LeCun, Yann and Boser, Bernhard and Denker, John and Henderson, Donnie and Howard, R. and Hubbard, Wayne and Jackel, Lawrence},
  booktitle = {Advances in Neural Information Processing Systems (NIPS 1989)},
  pages = {},
  publisher = {Morgan-Kaufmann},
  title = {Handwritten Digit Recognition with a Back-Propagation Network},
  url = {https://proceedings.neurips.cc/paper/1989/file/53c3bce66e43be4f209556518c2fcb54-Paper.pdf},
  volume = {2},
  year = {1990}
}

@inproceedings{10.5555/2969239.2969312,
  author = {Zhang, Xiang and Zhao, Junbo and LeCun, Yann},
  title = {Character-Level Convolutional Networks for Text Classification},
  year = {2015},
  publisher = {MIT Press},
  abstract = {This article offers an empirical exploration on the use of character-level convolutional networks (ConvNets) for text classification. We constructed several large-scale datasets to show that character-level convolutional networks could achieve state-of-the-art or competitive results. Comparisons are offered against traditional models such as bag of words, n-grams and their TFIDF variants, and deep learning models such as word-based ConvNets and recurrent neural networks.},
  booktitle = {Proceedings of the 28th International Conference on Neural Information Processing Systems (NIPS 2015)},
  url = {https://papers.nips.cc/paper/2015/file/250cf8b51c773f3f8dc8b4be867a9a02-Paper.pdf},
  pages = {649–657},
  numpages = {9},
  location = {Montreal, Canada},
  volume = {1}
}

@article{li_comparing_2009,
  author = {Li, Tian-Jun and Zhang, Weiyi},
  title = {Comparing tamed and compatible symplectic cones and cohomological properties of almost complex manifolds},
  eprint = {arXiv:0708.2520},
  journal = {arXiv:0708.2520 [math]},
  abstract = {We introduce certain homology and cohomology subgroups for any almost complex structure and study their pureness, fullness and duality properties. Motivated by a question of Donaldson, we use these groups to relate J-tamed symplectic cones and J-compatible symplectic cones over a large class of almost complex manifolds, including all Kahler manifolds, almost Kahler 4-manifolds and complex surfaces.},
  month = sep,
  year = {2009},
  keywords = {Mathematics - Algebraic Geometry, Mathematics - Differential Geometry, Mathematics - Symplectic Geometry},
}

@phdthesis{Gell-Mann:1951xrt,
  author = "Gell-Mann, Murray",
  title = "{Coupling Strength and Nuclear Reactions}",
  url = {https://dspace.mit.edu/handle/1721.1/12195},
  school = {MIT},
  year = "1951"
}

@Inbook{LeCun2012,
  author="LeCun, Yann A.
  and Bottou, L{\'e}on
  and Orr, Genevieve B.
  and M{\"u}ller, Klaus-Robert",
  editor="Montavon, Gr{\'e}goire
  and Orr, Genevi{\`e}ve B.
  and M{\"u}ller, Klaus-Robert",
  title="Efficient BackProp",
  bookTitle="Neural Networks: Tricks of the Trade: Second Edition",
  year="2012",
  publisher="Springer",
  address="Berlin, Heidelberg",
  pages="9--48",
  abstract="The convergence of back-propagation learning is analyzed so as to explain common phenomenon observed by practitioners. Many undesirable behaviors of backprop can be avoided with tricks that are rarely exposed in serious technical publications. This paper gives some of those tricks, and offers explanations of why they work.",
  isbn="978-3-642-35289-8",
  doi="10.1007/978-3-642-35289-8_3",
  url="https://doi.org/10.1007/978-3-642-35289-8_3"
}

@webpage{gage1994bpe,
  title = {A New Algorithm for Data Compression},
  author = {Philip Gage},
  url = {http://www.pennelynn.com/Documents/CUJ/HTML/94HTML/19940045.HTM},
  lastchecked = {2021-06-09}
}
